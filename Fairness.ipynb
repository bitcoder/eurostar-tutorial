{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AI Fairness\n",
    "\n",
    "Hold on to your data scientist hat but make room for your tester hat.  You'll need both for this section on AI/ML fairness. In these lessons and hands-on activities, you are going to learn how to assess ML models for fairness so that you can identify and quantify fairness-related risks, and employ strategies to mitigate them.  For the hands-on demos and exercises, you'll practice using data processing libraries like _pandas_ and _numpy_, along _seaborn_ for data visualization, to probe training data for fairness-related issues. You'll then get exposure to [AI Fairness 360](https://aif360.mybluemix.net/), an open-source toolkit for measuring and mitigating fairness-related risks. Since testing is about risk and risk mitigation, the AI/ML and software testing worlds intersect quite naturally here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Introduction to AI Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "AI is everywhere and in many ways this is great. However, at the same time, the opportunities brought about by AI also raise new challenges. Some of these challenges have received much attention in the media, and have highlighted the need to \"get AI right\".  Fairness in AI is about making sure that machines do not discriminate against certain groups of people, and possibly place already disadvantaged groups at a further disadvantage.  AI fairness is a fundamentally sociotechnical challenge, meaning that it cannot be approached from purely social or purely technical perspectives. Taken together, these factors can make it a pretty daunting landscape to navigate. Although there are few easy answers, there are a variety of strategies emerging for assessing and mitigating fairness-related risks, as well as a deepening understanding of the challenge throughout society. This tutorial explores these issues and gives you practice using open-source fairness toolkits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right; margin: 15px 15px 15px 15px;\" src=\"img/fairnessnotlaw.png\" width=\"350\"/>\n",
    "\n",
    "### Fairness Not Law\n",
    "\n",
    "Fairness is related, but distinct from anti-discrimination law. In this training, legal terminology like: _discriminate against_, _protected class_, _disparate treatment_ and _disparate impact_ are avoided. Although fairness is related to the concepts in anti-discrimination law, the goal is to focus on fairness and not touch on the legal considerations. It is important to note that some fairness interventions could be illegal, and, conversely, there are AI systems that follow the law, including antidiscrimination law, but still exhibit fairness issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<img style=\"float: right; margin: 15px 15px 15px 15px;\" src=\"img/biasintersection.png\" width=\"270\"/>\n",
    "\n",
    "### Impacts and Risks Instead of Bias\n",
    "\n",
    "If you’ve read anything about fairness in AI systems then you’ve probably seen the word _bias_ get thrown around. In it's textbook definition, **bias** is a systemic or disproportionate tendency toward something or someone. When it comes to AI, bias is often used as a _catch-all_ phrase for describing any unfair system behavior, or causes of that unfairness. This training avoids using the word bias whenever possible because the term is ambiguous. Bias means very different things to different communities. For example, there is statistical bias, social bias, and systemic bias to name a few.  However, most issues typically arise at the intersection of different types of bias.  For this reason, it is better to talk about the **impacts** or **risks** related to fairness in AI-based systems. Such terms are not only useful for referring to those who may be harmed by the system and in what ways, but also for describing assessment and mitigation strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Debiasing? No Such Thing\n",
    "I also want to emphasize that because there are so many different reasons why AI systems can cause fairness-related issues, it is not possible to completely remove bias from a system or guarantee its fairness. As a result, throughout this training you aren't likely to encounter terms like **debiasing** or **unbiased**, as they tend to set unrealistic expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluating Fairness in AI and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The goal of evaluating fairness in AI/ML is to answer the questions:\n",
    "* Are there groups of people who are disproportionately, negatively impacted by the system?\n",
    "* If so, in what ways are they impacted and what can we do about it?\n",
    "\n",
    "The steps in AI fairness validation are identifying risks or potential harm, determining which groups might be impacted, quantifying risks and comparing them across groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Identifying Risks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "There are three broad categories of fairness-related risks that can occur in ML models: **allocation risk**, **quality of service risk**, and **representation risk**.  Note that these are not mutually exclusive categories and therefore it is possible for a single AI system to exhibit more than one type of potentially harmful behavior.\n",
    "\n",
    "#### Allocation Risk\n",
    "\n",
    "This type of risk may occur when AI systems are used allocate opportunities or resources in ways that\n",
    "can have significant negative impacts on people’s lives. For example, Amazon abandoned its automated hiring system after finding that it amplified the existing gender imbalance in the tech industry by withholding employment opportunities\n",
    "from women.\n",
    "\n",
    "#### Quality of Service Risk\n",
    "Quality of service risk is about whether a system works as well for one person as it does for another. For example, researchers found that 3 commercial gender classifiers had higher error rates for images of women with darker skin tones, than for images of men with lighter skin tones. Like accessibility issues, quality of service risks can raise questions about respect, dignity, and personhood. Imagine how a user might feel if a system repeatedly fails to recognize her voice, but easily recognizes those of her peers?\n",
    "\n",
    "#### Representation Risk\n",
    "Representation risks include things like stereotyping, denigration, and any form of over- or under-representation. Denigration occurs when an AI system is itself part of a process that is actively derogatory, demeaning, or offensive. For example, Google Photos infamously mislabeled an image of Black people as _gorillas_. A similar scenario happened again a few years later when Facebook's AI labeled black men in a video as _primates_. This mislabeling is harmful not just because these system made the same mistake, but because the labels they applied have a history of being purposefully used to denigrate and demean Black people.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Determining Impacted Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "With potential fairness-related harms identified, it's time to determine who may be at risk of experiencing them. Many times it is the people who will use or operate the system, but somtimes it can be other stakeholders that are directly or indirectly impacted, either by choice or not. For example, in the case of a facial recognition system for workplace building access, the system operator is not the person whose face is being recognized and thus not the person who is most immediately harmed if the system makes a mistake.\n",
    "\n",
    "#### Search for the Most Relevant Groups\n",
    "Although news and media stories often focus on groups of people that are protected by antidiscrimination laws, such as groups defined in terms of race, gender, age, or disability status, there are actually many different groups of people that we want our systems to treat fairly and it’s not always easy to identify the most relevant ones, which can even be specific to the domain or use case. For example, in the case of an automated essay-grading system, whether someone is a native speaker of the language may be more relevant than their age or their disability status.\n",
    "\n",
    "#### Carefully Examine Group Intersections\n",
    "Remember that groups intersect and people at those intersections may be at risk of experiencing unique harms that might be obscured by considering only non-intersected groups. Returning to the image-based gender classifiers mentioned earlier, error rates were significantly higher for women with darker skin tones than for images of women overall, or for images of people with darker skin tones overall. The only way to discover those types of issues are consider the models' performance with respect to skin tone and gender at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Quantifying Risks and Comparing Across Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Risk quantification is a well-studied subject and you can probably find many tools and resources to help you with this step. The two key components of risk are the **likelihood** (probability) that a given event occurs, and the **severity** of any negative impacts. A common practice for quantifying risks is to use a **risk assessment matrix**.  As shown in the matrix, if there is a high probablity that a risk, if realized, will have a sever impact, then that item should have the attention of the team.  In software testing, the idea behind **risk-based testing** is that you should spend more time validating those aspects of the system that pose the highest threat, and such a philosophy is definitely applicable to testing AI for fairness.\n",
    "\n",
    "<img style=\"align: center; margin: 15px 15px 15px 15px;\" src=\"img/riskmatrix.png\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Two critical decisions when desiging an AI system are\n",
    "\n",
    "how we define the machine learning task\n",
    "what dataset we use to train our models\n",
    "These choices are often intertwined, because the dataset is often a convenience dataset, based on availability, which leads to a specific choice of label and performance metric (that's also the case in our scenario).\n",
    "\n",
    "In this part of the tutorial, we first load the dataset, and then we examine it for a variety of fairness issues:\n",
    "\n",
    "sample sizes of different demographic groups, and in particular different racial groups\n",
    "appropriateness of our choice of label (readmission within 30 days)\n",
    "representativeness/informativeness of different features for different groups\n",
    "Besides dataset characteristics, one additional aspect of dataset fairness is whether the data was collected in a manner that respects the autonomy of individuals in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Context for Hands-On Activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "The hands-on interactive demonstrations, practices, and exercises in this notebook are all based on the information in this section. Although each hands-on practice builds on the previous, any required code sections have been replicated within each section so that they are all self-contained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Consider an automated system for recommending patients for _high-risk care management_ programs.\n",
    "\n",
    "> These programs seek to improve the care of patients with complex health needs by providing additional resources, including greater attention from trained providers, to help ensure that care is well coordinated. Because the programs are themselves expensive with costs going toward teams of dedicated nurses, extra primary care appointment slots, and other scarce resources — **the automated system will rely extensively on ML algorithms to identify patients who will benefit the most**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development\n",
    "\n",
    "The proposed automated system in the scenario will be implemented as an ML classification model.\n",
    "\n",
    "> The purpose of the classifier is to predicts whether a patient should be suggested to their primary care physicians for enrollment into the care management program. A positive prediction will mean recommendation into the care program.  Since hospital **readmission within 30 days** can be viewed as a proxy that the patients needed more assistance at the release time, it has been selected as the label (output) the model will be trained to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset for this scenario is a [publicly available clinical dataset](https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008).\n",
    "\n",
    ">The data focuses on hospital re-admissions for _diabetic patients_ across 130 different hospitals in the U.S., recorded over a ten-year period. Each record represents a patient's readmission whose stay lasted one to fourteen days. The features describing each re-admission encounter include **demographics**, **diagnoses**, **diabetic medications**, **number of visits** in the preceding year, **payer information**, and whether the patient was **readmitted after release**, and if **readmission was within 30 days** of the release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hands-On with Testing Fairness in Model Training: Sample Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Setup\n",
    "\n",
    "#### Import and Configure Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing: numpy, pandas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.float_format\", \"{:.3f}\".format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/diabetic_preprocessed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task Description and Interactive Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Examine the dataset with an eye towards a common threat to fairness — the sample sizes for different demographic groups. \n",
    "\n",
    "#### Sample Sizes for Gender (Counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gender\"].value_counts().plot(kind='bar', rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Sample Sizes for Gender (Frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gender\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fairness Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Small sample sizes have two implications:\n",
    "* **Training**: Fewer training samples may result in the model failing to learn patterns related to smaller groups, which means that its predictive performance on these groups could be worse.\n",
    "* **Assessment**: Fewer data points overall may mean a much larger uncertainty (error bars) in model estimates, making the impacts of the system on smaller groups harder to assess. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Write code to **print** and **plot** the sample sizes of the **race** demographic group using either counts or frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Your Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Dataframe of Sample Sizes for Given Demographic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Dataframe Showing Sample Sizes for Given Demographic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sample Sizes Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your thoughts on the distribution for race?  \n",
    "\n",
    "Modify your code above so that you can examine sample sizes for age instead of race.   \n",
    "What risks if any are associated with age?  \n",
    "\n",
    "Using a scale of _high_, _medium_, _low_, rate the risks associated with each feature as pertains to sample sizes? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Mitigating the Risks of Small Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "When the data set lacks coverage of certain groups, it means that you won't be able to reliably assess any fairness-related issues. There are three possible interventions, which can be combined if needed. These are:\n",
    "\n",
    "* **Collecting**: gather more data for groups with fewer samples.\n",
    "* **Pooling**: merge some of the groups with fewer samples together to make a larger group.\n",
    "* **Dropping**: removing some of the group with fewer samples altogether (extra caution).\n",
    "\n",
    "The choice of strategy depends on your existing understanding of which groups are at the greatest risk of harm. Pooling the groups with widely different risks could mask the extent of harms. You should be extra  cautious against dropping small groups completely as this may leads to the representational harm of erasure. If any groups are merged or removed, these decisions should be annotated and explained. Experimentation is also a useful tool in your toolbelt, so I recommend to approach these issues as both a scientist and a software engineer/tester. For the previous exercise, the recommendations are:\n",
    "\n",
    "* Drop the gender group labeled *Unknown/Invalid* because the sample size is so small that no meaningful fairness assessment is possible.\n",
    "\n",
    "* Merge the three smallest race groups *Asian*, *Hispanic*, *Other*, but also retain the original groups for auxiliary assessments. \n",
    "\n",
    "Here is the code that implements these recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop gender group Unknown/Invalid\n",
    "df = df.query(\"gender != 'Unknown/Invalid'\")\n",
    "\n",
    "# retain the original race as race_all, and merge Asian+Hispanic+Other \n",
    "df[\"race_all\"] = df[\"race\"]\n",
    "df[\"race\"] = df[\"race\"].replace({\"Asian\": \"Other\", \"Hispanic\": \"Other\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hands-On with Fairness Toolkits: Mitigating Fairness Risks with AI Fairness 360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [AI Fairness 360](https://aif360.mybluemix.net/)\n",
    "\n",
    "AI Fairness 360 (AIF360) is an extensible open source toolkit that can help you examine, report, and mitigate fairness-related risks, referred to as **bias** on their site, in ML models. AI Fairness 360 was created by IBM Research and donated by IBM to the Linux Foundation AI & Data.\n",
    "\n",
    "AIF360 includes a [Python package](https://pypi.org/project/aif360/) that has a comprehensive set of metrics for datasets and models to test for biases, explanations for these metrics, and algorithms to mitigate bias in datasets and models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task Description and Interactive Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Since you have already explored the Python library for Fairlearn, in this hands-on task you'll interact with the AI Fairness 360 toolkit via a web-based demo.\n",
    "\n",
    "<img style=\"align: center; margin: 15px 15px 15px 15px; border:1px solid black;\"  src=\"img/aif360.gif\" width=\"800\"/>\n",
    "\n",
    "##### **Step 1.** Navigate to the [AIF360 Interactive Demo](https://aif360.mybluemix.net/data) website.\n",
    "\n",
    "##### **Step 2.** Walk through the process and apply mitigation strategies to different datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "What are your thoughts on some of the different mitigation strategies and the visualizations?\n",
    "* **Reweighing**: modifies the weights of different training examples\n",
    "* **Optimized Pre-Processing**: modifies training data labels and features\n",
    "* **Adversarial Debiasing**: leverages models that compete with each other\n",
    "* **Reject Option-Based Classification**: changes predictions from a classifier to make them fairer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
